#-------------------------------------------------------------------------
# AUTHOR: Henry Hu
# FILENAME: bagging_random_forest.py
# SPECIFICATION: Complete the python program, build a base classifier using
#   a single decision tree, ensemble classifier to combine multiple decision
#   trees, and a random forest classifier. Test the accuracy.
# FOR: CS 4210- Assignment #3
# TIME SPENT: 5 hours
#-----------------------------------------------------------*/

#IMPORTANT NOTE: DO NOT USE ANY ADVANCED PYTHON LIBRARY TO COMPLETE THIS CODE SUCH AS numpy OR pandas. You have to work here only with standard vectors and arrays

#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier

dbTraining = []
dbTest = []
X_training = []
y_training = []
classVotes = [] #this array will be used to count the votes of each classifier

try:
    # reading the training data and populate dbTraining
    reader = open('optdigits.tra')
    for row in reader:
        dbTraining.append(row)

    # reading the test data and populate dbTest
    reader = open('optdigits.tes')
    for row in reader:
        dbTest.append(row.rstrip().split(','))

finally:
    reader.close()


#inititalizing the class votes for each test sample. There should be 1797 samples.
for i in range(len(dbTest)):
    classVotes.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

print("Started my base and ensemble classifier ...")

error_count = 0

for k in range(20): #we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample

  bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)

  #populate the values of X_training and y_training by using the bootstrapSample
  for sample in bootstrapSample:

    # Take each boostrap sample, give the class label to Y_training, and the rest to X_training.
    sample_list = sample.split(',')
    y_training.append(int(sample_list.pop()))
    X_training.append(sample_list)


  #fitting the decision tree to the data
  clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None) #we will use a single decision tree without pruning it
  clf = clf.fit(X_training, y_training)

  for i, testSample in enumerate(dbTest):

      #make the classifier prediction for each test sample and update the corresponding index value in classVotes. For instance,
      # if your first base classifier predicted 2 for the first test sample, then classVotes[0,0,0,0,0,0,0,0,0,0] will change to classVotes[0,0,1,0,0,0,0,0,0,0].
      # Later, if your second base classifier predicted 3 for the first test sample, then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
      # Later, if your third base classifier predicted 3 for the first test sample, then classVotes[0,0,1,1,0,0,0,0,0,0] will change to classVotes[0,0,1,2,0,0,0,0,0,0]
      # this array will consolidate the votes of all classifier for all test samples

    # Input each test sample array minus the class label into the prediction. Tally it's vote. Do this for all 1797 samples.
    groundTruth = int(testSample[-1])
    prediction = int(clf.predict([testSample[:len(testSample)-1]])[0])
    classVotes[i][prediction] += 1

    if k == 0 and (prediction != groundTruth): #for only the first base classifier, compare the prediction with the true label of the test sample here to start calculating its accuracy
      error_count += 1

  if k == 0: #for only the first base classifier, print its accuracy here
     accuracy = 1 - error_count/float(len(dbTest))
     print("Finished my base classifier (fast but relatively low accuracy) ...")
     print("My base classifier accuracy: " + str(accuracy))
     print("")


  #now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)
ensemble_error_count = 0
for i, prediction_arr in enumerate(classVotes):
  # Finding the majority label
  majority_vote = 0
  cur_votes = -1

  for k in range(len(prediction_arr)):
    if prediction_arr[k] > cur_votes:
      majority_vote = k
      cur_votes = prediction_arr[k]

  # compare with the ground truth from test dataset.
  if majority_vote != int(dbTest[i][-1]):
    ensemble_error_count += 1

#printing the ensemble accuracy here
ensemble_accuracy = 1 - ensemble_error_count/float(len(dbTest))
print("Finished my ensemble classifier (slow but higher accuracy) ...")
print("My ensemble accuracy: " + str(ensemble_accuracy))
print("")

print("Started Random Forest algorithm ...")

  #Create a Random Forest Classifier
clf=RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

  #Fit Random Forest to the training data
clf.fit(X_training,y_training)

  #make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]

random_forest_error = 0

for testSample in dbTest:
  groundTruth = int(testSample[-1])
  prediction = int(clf.predict([testSample[:len(testSample) - 1]])[0])

  if prediction != groundTruth:
    random_forest_error += 1

  #compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
random_forest_accuracy = 1 - random_forest_error/float(len(dbTest))

  #printing Random Forest accuracy here
print("Random Forest accuracy: " + str(random_forest_accuracy))

print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
